## Test batch request mode on completions endpoint
POST http://localhost:8000/v1/completions
Content-Type: application/json

{
  "model": "Llama-3.2-1B-Instruct-4bit",
  "prompt": [
    "Translate the following English sentence to French: 'Hello, how are you?'",
    "How are our customers doing today?"
  ],
    "max_tokens": 60
}

###

## Test Logprobs:
POST http://localhost:8000/v1/completions
Content-Type: application/json

{
  "model": "TinyLlama-1.1B-Chat-v1.0",
  "prompt": "Translate the ",
  "temperature": 0,
    "logprobs": true,
    "max_tokens": 8,
    "top_p": 0.95
}