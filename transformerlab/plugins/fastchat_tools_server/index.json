{
    "name": "Fastchat Tools Server",
    "uniqueId": "fastchat_tools_server",
    "description": "Fastchat loads models for inference using Huggingface Transformers for generation.",
    "plugin-format": "python",
    "type": "loader",
    "version": "1.0.0",
    "model_architectures": [
      "CohereForCausalLM",
      "FalconForCausalLM",
      "GemmaForCausalLM",
      "Gemma2ForCausalLM",
      "GPTBigCodeForCausalLM",
      "LlamaForCausalLM",
      "MistralForCausalLM",
      "MixtralForCausalLM",
      "PhiForCausalLM",
      "Phi3ForCausalLM",
      "Qwen2ForCausalLM",
      "T5ForConditionalGeneration"
    ],
    "files": ["main.py", "run_server.py", "setup.sh"],
    "setup-script": "setup.sh",
    "parameters": {
      "num_gpus": {
        "title": "Number of GPUs",
        "type": "integer"
      },
      "eight_bit": {
        "title": "Enable 8-bit compression",
        "type": "boolean",
        "default": false
      }
    },
    "parameters_ui": {
      "num_gpus": {
        "ui:emptyValue": "",
        "ui:help": "Used to spread models over multiple GPUs. Leave empty to use default."
      }
    }
  }
  