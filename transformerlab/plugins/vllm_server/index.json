{
    "name": "vLLM Server",
    "uniqueId": "vllm_server",
    "description": "vLLM is a fast and easy-to-use library for LLM inference and serving.",
    "plugin-format": "python",
    "type": "loader",
    "version": "1.0.3",
    "model_architectures": [
        "CohereForCausalLM",
        "FalconForCausalLM",
        "GemmaForCausalLM",
        "GPTBigCodeForCausalLM",
        "LlamaForCausalLM",
        "MistralForCausalLM",
        "MixtralForCausalLM",
        "PhiForCausalLM",
        "Phi3ForCausalLM",
        "Qwen2ForCausalLM"
    ],
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": {
        "eight_bit": {
            "title": "8-bit",
            "type": "boolean",
            "default": false
        },
        "cpu_offload": {
            "title": "CPU Offload",
            "type": "boolean",
            "default": false
        }
    }
}